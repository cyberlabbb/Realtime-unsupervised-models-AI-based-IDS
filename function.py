from flask import Flask, send_from_directory
import time
from scapy.all import sniff, wrpcap
import os
import numpy as np
import pandas as pd
import subprocess
import tensorflow as tf
from tensorflow.keras.models import load_model
import joblib
from concurrent.futures import ThreadPoolExecutor
import threading
from flask import jsonify
from flask_socketio import SocketIO
import logging
from pathlib import Path
from keras.losses import MeanSquaredError
from keras.models import load_model
from flask_cors import CORS

os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__, static_folder=None)
CORS(app)
socketio = SocketIO(app, cors_allowed_origins="*")
executor = ThreadPoolExecutor(max_workers=4)
lock = threading.Lock()

BASE_DIR = Path(__file__).parent
OUTPUT_DIR = BASE_DIR / "pcap_splits"
CSV_OUTPUT_DIR = BASE_DIR / "csv_cicflowmeter"
CICFLOWMETER_DIR = BASE_DIR / "CICFlowMeter-4.0" / "bin"
CFM_PATH = CICFLOWMETER_DIR / "cfm.bat"
ALERT_DIR = BASE_DIR / "alerts"
CHUNK_SIZE = 5000

OUTPUT_DIR.mkdir(exist_ok=True)
CSV_OUTPUT_DIR.mkdir(exist_ok=True)
ALERT_DIR.mkdir(exist_ok=True)
CICFLOWMETER_DIR.mkdir(parents=True, exist_ok=True)

custom_objects = {"mse": MeanSquaredError()}
try:
    MODEL = load_model(BASE_DIR / "Model" / "autoencoder.h5", compile=False)
    SCALER = joblib.load(BASE_DIR / "Model" / "scaler.pkl")
    logger.info("ML models loaded successfully")
except Exception as e:
    logger.error(f"Failed to load ML models: {e}")
    raise

# Global variables
packet_count = 0
packet_buffer = []
file_index = 0


def extract_features_with_cicflowmeter(pcap_path: Path, output_dir: Path) -> Path:
    """Extract features from PCAP using CICFlowMeter"""
    if not CFM_PATH.exists():
        logger.error("CICFlowMeter not found at %s", CFM_PATH)
        return None

    try:
        cmd = [str(CFM_PATH), str(pcap_path), str(output_dir)]
        result = subprocess.run(
            cmd, check=True, cwd=str(CICFLOWMETER_DIR), capture_output=True, text=True
        )
        logger.debug("CICFlowMeter output: %s", result.stdout)
        logger.info("Extracted features from %s", pcap_path)

        csv_name = pcap_path.stem + ".pcap_Flow.csv"
        output_file = output_dir / csv_name

        if not output_file.exists():
            csv_files = list(output_dir.glob("*.csv"))
            if csv_files:
                output_file = csv_files[0]
            else:
                logger.error("No CSV file generated by CICFlowMeter")
                return None

        return output_file

    except subprocess.CalledProcessError as e:
        logger.error("CICFlowMeter failed with error: %s", e.stderr)
        return None


def aggregate_features(csv_file: Path) -> pd.DataFrame:
    """Aggregate network flow features for detection"""
    try:
        data = pd.read_csv(csv_file)
        if data.empty:
            logger.warning("Empty CSV file: %s", csv_file)
            return None

        # Define features and aggregations
        feature_config = {
            "Flow IAT Max": ["std"],
            "Fwd IAT Mean": ["mean", "std"],
            "Bwd IAT Std": ["mean"],
            "Idle Std": ["mean"],
        }

        # Check for missing columns
        missing = [col for col in feature_config if col not in data.columns]
        if missing:
            logger.warning("Missing expected columns: %s", missing)
            return None

        # Perform aggregation
        aggregated = {}
        for feature, aggs in feature_config.items():
            for agg_func in aggs:
                col_name = f"{feature}_{agg_func}"
                aggregated[col_name] = [data[feature].agg(agg_func)]

        features_df = pd.DataFrame(aggregated)
        expected_columns = [
            "Flow IAT Max_std",
            "Fwd IAT Mean_mean",
            "Fwd IAT Mean_std",
            "Bwd IAT Std_mean",
            "Idle Std_mean",
        ]

        return features_df.reindex(columns=expected_columns)

    except Exception as e:
        logger.error("Feature aggregation failed: %s", e)
        return None


def detect_anomalies(features: pd.DataFrame) -> np.ndarray:
    """Detect anomalies with proper feature scaling"""
    try:
        if features is None or features.empty:
            return np.array([])

        # Convert to numpy array
        feature_array = features.values.astype("float32")

        # Scale features
        features_scaled = SCALER.transform(feature_array)

        # Get reconstruction error
        reconstructions = MODEL.predict(features_scaled)
        test_loss = tf.keras.losses.mae(reconstructions, features_scaled).numpy()
        print("test_loss", test_loss)
        # Apply threshold
        threshold = 0.003
        predictions = (test_loss > threshold).astype(int)

        logger.debug("Anomaly predictions: %s", predictions)
        return predictions

    except Exception as e:
        logger.error("Anomaly detection failed: %s", e)
        return np.array([])
all_predictions = []

def process_packet_batch(buffer: list, index: int):
    """Process a batch of packets"""
    pcap_path = None
    csv_path = None

    try:
        pcap_path = OUTPUT_DIR / f"capture_{index}.pcap"
        wrpcap(str(pcap_path), buffer)
        logger.info("Saved PCAP: %s", pcap_path)

        csv_path = extract_features_with_cicflowmeter(pcap_path, CSV_OUTPUT_DIR)
        if not csv_path or not csv_path.exists():
            logger.warning("No features extracted for %s", pcap_path)
            return
        features = aggregate_features(csv_path)
        if features is None or features.empty:
            logger.warning("No features aggregated for %s", csv_path)
            return

        predictions = detect_anomalies(features)

        logger.info("Predictions: %s", predictions)

        batch_result = {
            "batch": index,
            "timestamp": time.time(),
            "predictions": predictions.tolist(),
        }
        all_predictions.append(batch_result)

        if predictions.any():
            handle_alert(predictions, buffer, index)

    except Exception as e:
        logger.error("Error processing batch %d: %s", index, e)

        logger.info("Predictions: %s", predictions)

        if predictions.any():
            handle_alert(predictions, buffer, index)

    except Exception as e:
        logger.error("Error processing batch %d: %s", index, e)
    finally:

        for f in [pcap_path, csv_path]:
            if f and f.exists():
                try:
                    f.unlink()
                except Exception as e:
                    logger.warning("Could not delete %s: %s", f, e)


def handle_alert(predictions: np.ndarray, packets: list, index: int):
    """Handle detected intrusion"""
    alert_count = np.sum(predictions)
    logger.warning("ðŸš¨ Detected %d anomalies in batch %d", alert_count, index)

    # Save alert data
    alert_dir = ALERT_DIR / f"alert_{index}"
    alert_dir.mkdir(exist_ok=True)

    alert_pcap = alert_dir / f"alert_{index}.pcap"
    wrpcap(str(alert_pcap), packets)

    socketio.emit(
        "intrusion_alert",
        {
            "timestamp": time.time(),
            "file_path": str(alert_pcap),
            "message": f"Detected {alert_count} anomalies",
            "severity": "high",
        },
    )


def handle_packet(packet):
    global packet_count, packet_buffer, file_index

    with lock:
        packet_buffer.append(packet)
        packet_count += 1

        if packet_count >= CHUNK_SIZE:
            current_buffer = packet_buffer.copy()
            current_index = file_index
            file_index += 1

            packet_buffer.clear()
            packet_count = 0

            executor.submit(process_packet_batch, current_buffer, current_index)
