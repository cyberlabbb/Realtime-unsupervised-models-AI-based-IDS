from datetime import datetime
from scapy.all import sniff, wrpcap, IP, TCP, UDP, ICMP
import time
import os
import numpy as np
import pandas as pd
import subprocess
import tensorflow as tf
from tensorflow.keras.models import load_model
import joblib
from concurrent.futures import ThreadPoolExecutor
import threading
import logging
from pathlib import Path
from keras.losses import MeanSquaredError
from pymongo import MongoClient
from dotenv import load_dotenv
from flask_socketio import SocketIO
from flask import Flask
import sys
from bson import ObjectId, json_util

# logging.basicConfig(
#     level=logging.WARNING,
#     format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
#     handlers=[logging.StreamHandler(), logging.FileHandler("app.log")],
# )
# logger = logging.getLogger(__name__)

from socket_instance import socketio, app


# Update logging configuration
logging.basicConfig(
    level=logging.WARNING,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout),  # Use stdout for Unicode support
        logging.FileHandler("app.log", encoding="utf-8"),  # Specify UTF-8 encoding
    ],
)
logger = logging.getLogger(__name__)
# Environment setup
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"

# Directory setup
BASE_DIR = Path(__file__).parent
OUTPUT_DIR = BASE_DIR / "pcap_splits"
CSV_OUTPUT_DIR = BASE_DIR / "csv_cicflowmeter"
CICFLOWMETER_DIR = BASE_DIR / "CICFlowMeter-4.0" / "bin"
CFM_PATH = CICFLOWMETER_DIR / "cfm.bat"
ALERT_DIR = BASE_DIR / "alerts"
MODEL_DIR = BASE_DIR / "Model"
CHUNK_SIZE = 5000

# Create required directories
for directory in [OUTPUT_DIR, CSV_OUTPUT_DIR, ALERT_DIR, CICFLOWMETER_DIR, MODEL_DIR]:
    directory.mkdir(parents=True, exist_ok=True)

# Load ML models
custom_objects = {"mse": MeanSquaredError()}
try:
    MODEL = load_model(MODEL_DIR / "autoencoder.h5", compile=False)
    SCALER = joblib.load(MODEL_DIR / "scaler.pkl")
    logger.info("ML models loaded successfully")
except Exception as e:
    logger.error(f"Failed to load ML models: {e}")
    raise

# Load environment variables
load_dotenv()

# MongoDB setup
try:
    client = MongoClient(os.environ.get("MONGO_URI"))
    db = client["network_monitor"]
    batches_collection = db["batches"]
    alerts_collection = db["alerts"]
    logger.info("Connected to MongoDB")
except Exception as e:
    logger.error(f"Failed to connect to MongoDB: {e}")
    raise

# Global state
packet_count = 0
packet_buffer = []
file_index = 0
all_predictions = []
executor = ThreadPoolExecutor(max_workers=4)
lock = threading.Lock()
packet_buffer_size = 1000 
sniff_thread = None
sniff_control = threading.Event()
is_sniffing = False

def extract_features_with_cicflowmeter(pcap_path: Path, output_dir: Path) -> Path:
    """Extract features from PCAP using CICFlowMeter"""
    if not CFM_PATH.exists():
        logger.error("CICFlowMeter not found at %s", CFM_PATH)
        return None

    try:
        cmd = [str(CFM_PATH), str(pcap_path), str(output_dir)]
        result = subprocess.run(
            cmd, check=True, cwd=str(CICFLOWMETER_DIR), capture_output=True, text=True
        )
        logger.info("Extracted features from %s", pcap_path)

        csv_name = pcap_path.stem + ".pcap_Flow.csv"
        output_file = output_dir / csv_name

        if not output_file.exists():
            csv_files = list(output_dir.glob("*.csv"))
            if csv_files:
                output_file = csv_files[0]
            else:
                logger.error("No CSV file generated by CICFlowMeter")
                return None

        return output_file
    except subprocess.CalledProcessError as e:
        logger.error("CICFlowMeter failed: %s", e.stderr)
        return None


def aggregate_features(csv_file: Path) -> pd.DataFrame:
    """Aggregate network flow features"""
    try:
        data = pd.read_csv(csv_file)
        if data.empty:
            logger.warning("Empty CSV file: %s", csv_file)
            return None

        feature_config = {
            "Flow IAT Max": ["std"],
            "Fwd IAT Mean": ["mean", "std"],
            "Bwd IAT Std": ["mean"],
            "Idle Std": ["mean"],
        }

        # Check for missing columns
        missing = [col for col in feature_config if col not in data.columns]
        if missing:
            logger.warning("Missing columns: %s", missing)
            return None

        # Perform aggregation
        aggregated = {}
        for feature, aggs in feature_config.items():
            for agg_func in aggs:
                col_name = f"{feature}_{agg_func}"
                aggregated[col_name] = [data[feature].agg(agg_func)]

        return pd.DataFrame(aggregated).reindex(
            columns=[
                "Flow IAT Max_std",
                "Fwd IAT Mean_mean",
                "Fwd IAT Mean_std",
                "Bwd IAT Std_mean",
                "Idle Std_mean",
            ]
        )
    except Exception as e:
        logger.error("Feature aggregation failed: %s", e)
        return None


def detect_anomalies(features: pd.DataFrame) -> np.ndarray:
    """Detect anomalies with autoencoder"""
    try:
        if features is None or features.empty:
            return np.array([])

        feature_array = features.values.astype("float32")
        features_scaled = SCALER.transform(feature_array)
        reconstructions = MODEL.predict(features_scaled)
        test_loss = tf.keras.losses.mae(reconstructions, features_scaled).numpy()

        threshold = 0.003
        predictions = (test_loss > threshold).astype(int)
        logger.debug("Anomaly predictions: %s", predictions)
        return predictions
    except Exception as e:
        logger.error("Anomaly detection failed: %s", e)
        return np.array([])


def analyze_packet_stats(packets):
    """Analyze packet statistics"""
    stats = {
        "total_packets": len(packets),
        "total_bytes": 0,
        "protocol_distribution": {"TCP": 0, "UDP": 0, "ICMP": 0, "Other": 0},
        "flag_count": {
            flag: 0 for flag in ["SYN", "ACK", "FIN", "RST", "PSH", "URG", "ECE", "CWR"]
        },
        "src_ips": set(),
        "dst_ips": set(),
        "start_time": None,
        "end_time": None,
    }

    if not packets:
        return stats

    stats["start_time"] = datetime.fromtimestamp(float(packets[0].time))
    stats["end_time"] = datetime.fromtimestamp(float(packets[-1].time))

    for pkt in packets:
        stats["total_bytes"] += len(pkt)

        if pkt.haslayer(TCP):
            stats["protocol_distribution"]["TCP"] += 1
            tcp_layer = pkt.getlayer(TCP)
            for flag, mask in [
                ("SYN", 0x02),
                ("ACK", 0x10),
                ("FIN", 0x01),
                ("RST", 0x04),
                ("PSH", 0x08),
                ("URG", 0x20),
                ("ECE", 0x40),
                ("CWR", 0x80),
            ]:
                if tcp_layer.flags & mask:
                    stats["flag_count"][flag] += 1
        elif pkt.haslayer(UDP):
            stats["protocol_distribution"]["UDP"] += 1
        elif pkt.haslayer(ICMP):
            stats["protocol_distribution"]["ICMP"] += 1
        else:
            stats["protocol_distribution"]["Other"] += 1

        if pkt.haslayer(IP):
            stats["src_ips"].add(pkt[IP].src)
            stats["dst_ips"].add(pkt[IP].dst)

    stats["src_ip_count"] = len(stats["src_ips"])
    stats["dst_ip_count"] = len(stats["dst_ips"])
    stats["average_packet_size"] = (
        stats["total_bytes"] / stats["total_packets"]
        if stats["total_packets"] > 0
        else 0
    )

    del stats["src_ips"]
    del stats["dst_ips"]

    return stats


def save_batch_to_db(pcap_path, packets, index):
    """Save batch to MongoDB"""
    try:
        stats = analyze_packet_stats(packets)
        batch_doc = {
            "batch_name": f"batch_{index}",
            "created_at": datetime.now(),
            "pcap_file_path": str(pcap_path),
            **stats,
            "note": f"Processed at {datetime.now().isoformat()}",
        }
        result = batches_collection.insert_one(batch_doc)
        logger.info(f"Saved batch {index} to MongoDB")
        return result.inserted_id
    except Exception as e:
        logger.error(f"Failed to save batch {index}: {e}")
        return None


def process_packet_batch(buffer: list, index: int):
    """Process packet batch"""
    pcap_path = None
    csv_path = None

    try:
        pcap_path = OUTPUT_DIR / f"capture_{index}.pcap"
        wrpcap(str(pcap_path), buffer)
        logger.info("Saved PCAP: %s", pcap_path)

        batch_id = save_batch_to_db(pcap_path, buffer, index)
        if not batch_id:
            logger.warning("Failed to save batch info")

        csv_path = extract_features_with_cicflowmeter(pcap_path, CSV_OUTPUT_DIR)
        if not csv_path:
            return

        features = aggregate_features(csv_path)
        if features is None:
            return

        predictions = detect_anomalies(features)
        logger.info("Predictions: %s", predictions)

        batch_result = {
            "batch": index,
            "timestamp": time.time(),
            "predictions": predictions.tolist(),
            "batch_id": str(batch_id) if batch_id else None,
        }
        all_predictions.append(batch_result)

        if predictions.any():
            handle_alert(predictions, buffer, index, batch_id, csv_path)

    except Exception as e:
        logger.error("Error processing batch %d: %s", index, e)
    finally:
        for f in [pcap_path, csv_path]:
            if f and f.exists():
                try:
                    f.unlink()
                except Exception as e:
                    logger.warning("Could not delete %s: %s", f, e)


def handle_alert(
    predictions: np.ndarray,
    packets: list,
    index: int,
    batch_id: str = None,
    csv_path: Path = None,
):

    """Handle intrusion alert"""
    alert_count = np.sum(predictions)
    logger.warning("ALERT: Detected %d anomalies in batch %d", alert_count, index)

    alert_dir = ALERT_DIR / f"alert_{index}"
    alert_dir.mkdir(exist_ok=True)
    alert_pcap = alert_dir / f"alert_{index}.pcap"
    wrpcap(str(alert_pcap), packets)
    alert_csv = None
    if csv_path and csv_path.exists():
        alert_csv = alert_dir / csv_path.name
        try:
            csv_path.rename(alert_csv)  # Di chuyển file CSV vào alert folder
        except Exception as e:
            logger.warning(f"Could not move CSV file: {e}")

    # Create alert document
    alert_doc = {
        "timestamp": datetime.now(),
        "batch_id": str(batch_id) if batch_id else None,  # Convert ObjectId to string
        "batch_index": index,
        "alert_count": int(alert_count),
        "pcap_path": str(alert_pcap),
        "message": f"Detected {alert_count} anomalies",
        "severity": "high",
        "processed": False,
        "csv_path": str(alert_csv) if alert_csv else None,
    }

    try:
        # Save to MongoDB
        result = alerts_collection.insert_one(alert_doc)
        alert_id = str(result.inserted_id)  # Convert ObjectId to string
        logger.info(f"Saved alert to MongoDB with ID: {alert_id}")

        # Emit alert with more details - ensure all data is JSON serializable
        socketio.emit(
            "intrusion_alert",
            {
                "id": alert_id,
                "timestamp": time.time(),
                "file_path": str(alert_pcap),
                "message": f"Detected {alert_count} anomalies",
                "severity": "high",
                "batch_id": (
                    str(batch_id) if batch_id else None
                ),  # Convert ObjectId to string
                "batch_index": index,
                "alert_count": int(alert_count),
                "progress": {
                    "current": index,
                    "total": file_index,
                    "percentage": round(
                        (index / file_index * 100) if file_index > 0 else 0, 2
                    ),
                },
            },
        )
        logger.debug(f"Emitted alert notification for batch {index}")
    except Exception as e:
        logger.error(f"Failed to handle alert: {str(e)}")  # Convert error to string
        raise  # Re-raise the exception for proper error handling


def handle_packet(packet):
    global packet_count, packet_buffer, file_index

    with lock:
        packet_data = None
        try:
            if packet.haslayer(IP):
                packet_data = {
                    "timestamp": datetime.now().isoformat(),
                    "src_ip": packet[IP].src,
                    "dst_ip": packet[IP].dst,
                    "protocol": packet[IP].proto,
                    "length": len(packet),
                    "info": packet.summary(),
                }
                # Emit only once if packet data is valid
                if packet_data:
                    socketio.emit("new_packet", packet_data)
                    logger.info(f"Emitted packet: {packet_data}")
        except Exception as e:
            logger.error(f"Error emitting packet: {e}")

        # Always add to buffer regardless of emit success
        packet_buffer.append(packet)
        packet_count += 1

        if packet_count >= CHUNK_SIZE:
            current_buffer = packet_buffer.copy()
            current_index = file_index
            file_index += 1

            packet_buffer.clear()
            packet_count = 0

            executor.submit(process_packet_batch, current_buffer, current_index)
