from datetime import datetime
from scapy.all import sniff, wrpcap, IP, TCP, UDP, ICMP
import time
import os
import numpy as np
import pandas as pd
import subprocess
import tensorflow as tf
from tensorflow.keras.models import load_model
import joblib
from concurrent.futures import ThreadPoolExecutor
import threading
import logging
from pathlib import Path
from keras.losses import MeanSquaredError
from pymongo import MongoClient
from dotenv import load_dotenv
from flask_socketio import SocketIO
from flask import Flask
import sys
from bson import ObjectId, json_util
import shutil
from datetime import datetime
import pytz
import shutil
from pathlib import Path
import pytz
from datetime import datetime
from model_state import get_model  # thay tháº¿ biáº¿n toÃ n cá»¥c

# logging.basicConfig(
#     level=logging.WARNING,
#     format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
#     handlers=[logging.StreamHandler(), logging.FileHandler("app.log")],
# )
# logger = logging.getLogger(__name__)

from socket_instance import socketio, app


# Update logging configuration
logging.basicConfig(
    level=logging.WARNING,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout),  # Use stdout for Unicode support
        logging.FileHandler("app.log", encoding="utf-8"),  # Specify UTF-8 encoding
    ],
)
logger = logging.getLogger(__name__)
# Environment setup
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"

# Directory setup
BASE_DIR = Path(__file__).parent
OUTPUT_DIR = BASE_DIR / "pcap_splits"
CSV_OUTPUT_DIR = BASE_DIR / "csv_cicflowmeter"
CICFLOWMETER_DIR = BASE_DIR / "CICFlowMeter-4.0" / "bin"
CFM_PATH = CICFLOWMETER_DIR / "cfm.bat"
BATCH_DIR = BASE_DIR / "batches"
BATCH_DIR.mkdir(parents=True, exist_ok=True)
MODEL_DIR = BASE_DIR / "Model"
CHUNK_SIZE = 5000

for directory in [OUTPUT_DIR, CSV_OUTPUT_DIR, BATCH_DIR, CICFLOWMETER_DIR, MODEL_DIR]:
    directory.mkdir(parents=True, exist_ok=True)


custom_objects = {"mse": MeanSquaredError()}
try:
    AU_MODEL = load_model(MODEL_DIR / "autoencoder.h5", compile=False)
    KMEANS_MODEL = joblib.load(MODEL_DIR / "kmeans_model.pkl")
    SVM_MODEL = joblib.load(MODEL_DIR / "svm_model.pkl")

    ANOMALY_DATA = np.load(MODEL_DIR / "autoencoder_train_info.npz")
    AUTOENCODER_THRESHOLD = ANOMALY_DATA["threshold"].item()

    SCALER_AU = joblib.load(MODEL_DIR / "scaler_autoencoder.pkl")
    SCALER_KMEANS = joblib.load(MODEL_DIR / "scaler_kmeans.pkl")
    SCALER_SVM = joblib.load(MODEL_DIR / "scaler_svm.pkl")
    logger.info("ML models loaded successfully")
except Exception as e:
    logger.error(f"Failed to load ML models: {e}")
    raise

# Load environment variables
load_dotenv()

# MongoDB setup
try:
    client = MongoClient(os.environ.get("MONGO_URI"))
    db = client["network_monitor"]
    batches_collection = db["batches"]
    alerts_collection = db["alerts"]
    logger.info("Connected to MongoDB")
except Exception as e:
    logger.error(f"Failed to connect to MongoDB: {e}")
    raise

# Global state
packet_count = 0
packet_buffer = []
file_index = 0
all_predictions = []
executor = ThreadPoolExecutor(max_workers=4)
lock = threading.Lock()
packet_buffer_size = 1000
sniff_thread = None
sniff_control = threading.Event()
is_sniffing = False


def extract_features_with_cicflowmeter(pcap_path: Path, output_dir: Path) -> Path:
    """Extract features from PCAP using CICFlowMeter"""
    if not CFM_PATH.exists():
        logger.error("CICFlowMeter not found at %s", CFM_PATH)
        return None

    try:
        cmd = [str(CFM_PATH), str(pcap_path), str(output_dir)]
        result = subprocess.run(
            cmd, check=True, cwd=str(CICFLOWMETER_DIR), capture_output=True, text=True
        )
        logger.info("Extracted features from %s", pcap_path)

        csv_name = pcap_path.stem + ".pcap_Flow.csv"
        output_file = output_dir / csv_name

        if not output_file.exists():
            csv_files = list(output_dir.glob("*.csv"))
            if csv_files:
                output_file = csv_files[0]
            else:
                logger.error("No CSV file generated by CICFlowMeter")
                return None

        return output_file
    except subprocess.CalledProcessError as e:
        logger.error("CICFlowMeter failed: %s", e.stderr)
        return None


def aggregate_features(csv_file: Path) -> pd.DataFrame:
    """Aggregate network flow features"""
    try:
        data = pd.read_csv(csv_file)
        if data.empty:
            logger.warning("Empty CSV file: %s", csv_file)
            return None

        model = get_model()
        logger.info(f"Aggregating features for model: {model}")

        if model == "autoencoder":
            selected_cols = [
                "Flow Duration_mean",
                "Fwd IAT Tot_std",
                "Fwd IAT Max_std",
                "Fwd IAT Std_mean",
                "Fwd IAT Std_std",
                "Bwd IAT Max_mean",
            ]
            config = {
                "Flow Duration": ["mean"],
                "Fwd IAT Tot": ["std"],
                "Fwd IAT Max": ["std"],
                "Fwd IAT Std": ["mean", "std"],
                "Bwd IAT Max": ["mean"],
            }
        elif model == "kmeans":
            selected_cols = [
                "Flow Duration_mean",
                "Fwd IAT Tot_std",
                "Fwd IAT Max_std",
                "Fwd IAT Std_mean",
                "Fwd IAT Std_std",
                "Bwd IAT Max_mean",
            ]
            config = {
                "Flow Duration": ["mean"],
                "Fwd IAT Tot": ["std"],
                "Fwd IAT Max": ["std"],
                "Fwd IAT Std": ["mean", "std"],
                "Bwd IAT Max": ["mean"],
            }
        elif model == "svm":
            selected_cols = [
                "Flow Duration_mean",
                "Fwd IAT Tot_mean",
                "Fwd IAT Tot_std",
                "Bwd IAT Max_mean",
                "Bwd IAT Std_mean",
            ]
            config = {
                "Flow Duration": ["mean"],
                "Fwd IAT Tot": ["mean", "std"],
                "Bwd IAT Max": ["mean"],
                "Bwd IAT Std": ["mean"],
            }
        else:
            logger.error(f"Unknown model: {model}")
            return None

        # Check for missing columns
        missing = [col for col in config if col not in data.columns]
        if missing:
            logger.warning("Missing columns in CSV: %s", missing)
            return None

        # Aggregation
        aggregated = {}
        for col, aggs in config.items():
            for agg in aggs:
                key = f"{col}_{agg}"
                aggregated[key] = [data[col].agg(agg)]

        df = pd.DataFrame(aggregated).reindex(columns=selected_cols)
        df = df.astype(np.float32)
        logger.debug(f"Aggregated features: {df}")
        return df

    except Exception as e:
        logger.error("Feature aggregation failed: %s", e)
        return None


def detect_anomalies_AU(features: pd.DataFrame) -> np.ndarray:
    try:
        if features is None or features.empty:
            return np.array([])

        features = features.astype("float32")
        features_scaled = SCALER_AU.transform(features)

        def predict(model, data, threshold):
            reconstructions = model.predict(data)
            loss = tf.keras.losses.mae(reconstructions, data)
            return (loss.numpy() < threshold).astype(int)

        preds = predict(AU_MODEL, features_scaled, AUTOENCODER_THRESHOLD)
        logger.debug("Anomaly predictions: %s", preds.tolist())
        return preds

    except Exception as e:
        logger.error("Anomaly detection failed: %s", e)
        return np.array([])


def detect_anomalies_KMEANS(features: pd.DataFrame) -> np.ndarray:
    """Detect anomalies using KMeans model"""
    try:
        if features is None or features.empty:
            return np.array([])

        expected_columns = [
            "Flow Duration_mean",
            "Fwd IAT Tot_std",
            "Fwd IAT Max_std",
            "Fwd IAT Std_mean",
            "Fwd IAT Std_std",
            "Bwd IAT Max_mean",
        ]

        features = features[expected_columns]
        features.replace([np.inf, -np.inf], np.nan, inplace=True)
        features.fillna(0, inplace=True)

        features_scaled = SCALER_KMEANS.transform(features)
        features_scaled_df = pd.DataFrame(features_scaled, columns=features.columns)

        # ðŸ‘‡ Predict then map 0 â†’ 1 (anomaly), 1 â†’ 0 (benign)
        raw_predictions = KMEANS_MODEL.predict(features_scaled_df)
        predictions = np.array([1 if pred == 0 else 0 for pred in raw_predictions])

        logger.debug(f"Predictions: {predictions}")
        return predictions

    except Exception as e:
        logger.error(f"KMeans detection failed: {str(e)}")
        return np.array([])


def detect_anomalies_SVM(features: pd.DataFrame) -> np.ndarray:
    try:
        if features is None or features.empty:
            return np.array([])

        logger.error(f"Features shape: {features.shape}")
        logger.error(f"Features columns: {features.columns}")
        logger.error(f"Features dtypes: {features.dtypes}")

        features = features.astype(np.float32)
        predictions = SVM_MODEL.predict(features)
        return predictions
    except Exception as e:
        logger.error("Anomaly detection failed: %s", e)
        return np.array([])


def analyze_packet_stats(packets):
    """Analyze packet statistics"""
    stats = {
        "total_packets": len(packets),
        "total_bytes": 0,
        "protocol_distribution": {"TCP": 0, "UDP": 0, "ICMP": 0, "Other": 0},
        "flag_count": {
            flag: 0 for flag in ["SYN", "ACK", "FIN", "RST", "PSH", "URG", "ECE", "CWR"]
        },
        "src_ips": set(),
        "dst_ips": set(),
        "start_time": None,
        "end_time": None,
    }

    if not packets:
        return stats

    stats["start_time"] = datetime.fromtimestamp(float(packets[0].time))
    stats["end_time"] = datetime.fromtimestamp(float(packets[-1].time))

    for pkt in packets:
        stats["total_bytes"] += len(pkt)

        if pkt.haslayer(TCP):
            stats["protocol_distribution"]["TCP"] += 1
            tcp_layer = pkt.getlayer(TCP)
            for flag, mask in [
                ("SYN", 0x02),
                ("ACK", 0x10),
                ("FIN", 0x01),
                ("RST", 0x04),
                ("PSH", 0x08),
                ("URG", 0x20),
                ("ECE", 0x40),
                ("CWR", 0x80),
            ]:
                if tcp_layer.flags & mask:
                    stats["flag_count"][flag] += 1
        elif pkt.haslayer(UDP):
            stats["protocol_distribution"]["UDP"] += 1
        elif pkt.haslayer(ICMP):
            stats["protocol_distribution"]["ICMP"] += 1
        else:
            stats["protocol_distribution"]["Other"] += 1

        if pkt.haslayer(IP):
            stats["src_ips"].add(pkt[IP].src)
            stats["dst_ips"].add(pkt[IP].dst)

    stats["src_ip_count"] = len(stats["src_ips"])
    stats["dst_ip_count"] = len(stats["dst_ips"])
    stats["average_packet_size"] = (
        stats["total_bytes"] / stats["total_packets"]
        if stats["total_packets"] > 0
        else 0
    )

    del stats["src_ips"]
    del stats["dst_ips"]

    return stats


def save_batch_to_db(pcap_path, packets, index, is_attack=False, csv_path: Path = None):
    """Save batch to MongoDB with proper file handling"""
    try:
        stats = analyze_packet_stats(packets)
        vietnam_tz = pytz.timezone("Asia/Ho_Chi_Minh")
        current_time = datetime.now(vietnam_tz)

        timestamp_str = current_time.strftime("%Y%m%d_%H%M%S")
        batch_name = f"batch_{timestamp_str}"
        batch_dir = BATCH_DIR / batch_name
        batch_dir.mkdir(parents=True, exist_ok=True)

        pcap_file = batch_dir / f"{batch_name}.pcap"
        wrpcap(str(pcap_file), packets)

        batch_csv = None
        if csv_path and csv_path.exists():
            batch_csv = batch_dir / f"{batch_name}.csv"
            try:
                shutil.copy2(str(csv_path), str(batch_csv))
            except Exception as e:
                logger.warning(f"Could not copy CSV file: {e}")

        batch_doc = {
            "batch_name": batch_name,
            "created_at": current_time,
            "pcap_file_path": str(pcap_file),
            "csv_file_path": str(batch_csv) if batch_csv else None,
            **stats,
            "note": f"Processed at {current_time.isoformat()}",
            "is_attack": is_attack,
        }

        result = batches_collection.insert_one(batch_doc)
        batch_id = result.inserted_id

        socket_batch = {
            **batch_doc,
            "_id": {"$oid": str(batch_id)},
            "created_at": batch_doc["created_at"].isoformat(),
            "start_time": (
                batch_doc["start_time"].isoformat()
                if batch_doc.get("start_time")
                else None
            ),
            "end_time": (
                batch_doc["end_time"].isoformat() if batch_doc.get("end_time") else None
            ),
        }

        socketio.emit("new_batch", socket_batch)

        if is_attack:
            alert_data = {
                "batch_id": str(batch_id),
                "message": f"Attack detected in batch {batch_name}",
                "severity": "high",
                "timestamp": current_time.isoformat(),
            }
            socketio.emit("intrusion_alert", alert_data)

        return batch_id

    except Exception as e:
        logger.error(f"Failed to save batch {index}: {e}")
        return None


def process_packet_batch(buffer: list, index: int):
    """Process packet batch with improved file handling"""
    model = get_model()

    pcap_path = None
    csv_path = None

    try:
        pcap_path = OUTPUT_DIR / f"temp_capture_{index}.pcap"
        wrpcap(str(pcap_path), buffer)
        csv_path = extract_features_with_cicflowmeter(pcap_path, CSV_OUTPUT_DIR)

        features = aggregate_features(csv_path) if csv_path else None
        logger.info(f"Using model: {model} for predictions")

        if model == "autoencoder":
            predictions = (
                detect_anomalies_AU(features) if features is not None else np.array([])
            )
        elif model == "kmeans":
            predictions = (
                detect_anomalies_KMEANS(features)
                if features is not None
                else np.array([])
            )
        elif model == "svm":
            predictions = (
                detect_anomalies_SVM(features) if features is not None else np.array([])
            )
        else:
            logger.error(f"Invalid model selected: {model}")
            predictions = np.array([])

        logger.info(f"Using model: {model} for predictions")

        is_attack = bool(predictions.any())

        batch_id = save_batch_to_db(pcap_path, buffer, index, is_attack, csv_path)

        if features is not None:
            try:
                df = pd.read_csv(csv_path)
                if not df.empty:
                    flow_dicts = df.replace({np.nan: None}).to_dict(orient="records")
                    for flow in flow_dicts:
                        flow["batch_index"] = index
                    flows_collection = db["flows"]
                    flows_collection.insert_many(flow_dicts)
                    logger.info(f"Inserted {len(flow_dicts)} flows for batch {index}")
            except Exception as e:
                logger.error(f"Failed to insert flows for batch {index}: {e}")

        batch_result = {
            "batch": index,
            "timestamp": time.time(),
            "predictions": predictions.tolist(),
            "batch_id": str(batch_id) if batch_id else None,
        }
        all_predictions.append(batch_result)

    except Exception as e:
        logger.error("Error processing batch %d: %s", index, e)
    finally:

        for temp_file in [pcap_path, csv_path]:
            if temp_file and temp_file.exists():
                try:
                    temp_file.unlink()
                except Exception as e:
                    logger.warning(f"Could not delete temporary file {temp_file}: {e}")


def handle_packet(packet):
    global packet_count, packet_buffer, file_index

    with lock:
        packet_data = None
        try:
            if packet.haslayer(IP):
                vietnam_tz = pytz.timezone("Asia/Ho_Chi_Minh")
                vn_time = datetime.now(vietnam_tz)

                packet_data = {
                    "timestamp": vn_time.isoformat(),
                    "src_ip": packet[IP].src,
                    "dst_ip": packet[IP].dst,
                    "protocol": packet[IP].proto,
                    "length": len(packet),
                    "info": packet.summary(),
                }
               
                if packet_data:
                    socketio.emit("new_packet", packet_data)
                    logger.info(f"Emitted packet: {packet_data}")
        except Exception as e:
            logger.error(f"Error emitting packet: {e}")
        packet_buffer.append(packet)
        packet_count += 1

        if packet_count >= CHUNK_SIZE:
            current_buffer = packet_buffer.copy()
            current_index = file_index
            file_index += 1

            packet_buffer.clear()
            packet_count = 0

            executor.submit(process_packet_batch, current_buffer, current_index)
